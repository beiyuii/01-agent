"""å²—ä½æ•°æ® ETL è„šæœ¬ï¼Œè´Ÿè´£æ•°æ®æ¸…æ´—ã€å‘é‡åŒ–åŠæŒä¹…åŒ–å…¥åº“ã€‚"""

from pathlib import Path
import shutil
import sys

import pandas as pd
from langchain.text_splitter import CharacterTextSplitter

ROOT_DIR = Path(__file__).resolve().parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from app.core.config import settings
from app.services import get_vector_store


# è„šæœ¬åŠŸèƒ½ï¼š
# 1. è¯»å–æ•°æ®
# 2. æ•°æ®æ¸…æ´—
# 3. æ•°æ®è½¬æ¢
# 4. æ•°æ®å­˜å‚¨

# è¯»å–æ•°æ®å¹¶è¿›è¡Œæ¸…æ´—
def load_clean_data(path: str) -> pd.DataFrame:
    """åŠ è½½åŸå§‹æ•°æ®å¹¶è¿›è¡Œç¼ºå¤±å€¼æ¸…æ´—ã€‚

    Args:
        path (str): åŸå§‹ CSV/Excel æ–‡ä»¶è·¯å¾„ã€‚

    Returns:
        pd.DataFrame: æ¸…æ´—åçš„æ ‡å‡†åŒ–æ•°æ®è¡¨ã€‚
    """

    expected_columns = [
        "åºå·",
        "å…¬å¸åç§°",
        "æ‰¹æ¬¡",
        "ä¼ä¸šæ€§è´¨",
        "è¡Œä¸šå¤§ç±»",
        "æ‹›è˜å¯¹è±¡",
        "æ‹›è˜å²—ä½",
        "ç½‘ç”³çŠ¶æ€",
        "å·¥ä½œåœ°ç‚¹",
        "æ›´æ–°æ—¶é—´",
        "æˆªæ­¢æ—¶é—´",
        "å®˜æ–¹å…¬å‘Š",
        "æŠ•é€’æ–¹å¼",
        "å†…æ¨ç |å¤‡æ³¨",
    ]

    if str(path).endswith(".csv"):
        df = pd.read_csv(
            path,
            encoding="utf-8-sig",
            usecols=range(len(expected_columns)),
            names=expected_columns,
            header=0,
        )
    else:
        df = pd.read_excel(path, usecols=expected_columns)

    # è½¬åŒ–è¡¨å¤´ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆå»é™¤ç©ºæ ¼å¹¶æŒ‰é¢„æœŸé¡ºåºæ’åˆ—ï¼‰
    df.columns = df.columns.map(lambda col: str(col).strip())

    missing_columns = [col for col in expected_columns if col not in df.columns]
    if missing_columns:
        raise ValueError(
            "æ•°æ®è¡¨ç¼ºå°‘é¢„æœŸå­—æ®µï¼Œè¯·æ£€æŸ¥æ•°æ®æº: " + ", ".join(missing_columns)
        )

    df = df[expected_columns]

    # æ•°æ®æ¸…æ´—
    # 1. å¯¹é‡è¦æ•°æ®ç¼ºå¤±çš„è¿›è¡Œåˆ é™¤
    df = df.dropna(subset=["å…¬å¸åç§°", "æ‹›è˜å²—ä½"])

    # 2. é‡è¦çš„æ•°æ®ä¸­å¯¹ç©ºå€¼ã€å¼‚å¸¸å€¼è¿›è¡Œå¡«å……å¤„ç†
    df["æ‰¹æ¬¡"] = df["æ‰¹æ¬¡"].fillna("æœªçŸ¥")
    df["ä¼ä¸šæ€§è´¨"] = df["ä¼ä¸šæ€§è´¨"].fillna("æœªçŸ¥")
    df["è¡Œä¸šå¤§ç±»"] = df["è¡Œä¸šå¤§ç±»"].fillna("æœªçŸ¥")
    df["æ‹›è˜å¯¹è±¡"] = df["æ‹›è˜å¯¹è±¡"].fillna("æœªçŸ¥")
    df["æ‹›è˜å²—ä½"] = df["æ‹›è˜å²—ä½"].fillna("æœªçŸ¥")
    df["ç½‘ç”³çŠ¶æ€"] = df["ç½‘ç”³çŠ¶æ€"].fillna("æœªçŸ¥")
    df["å·¥ä½œåœ°ç‚¹"] = df["å·¥ä½œåœ°ç‚¹"].fillna("æœªçŸ¥")
    df["æ›´æ–°æ—¶é—´"] = df["æ›´æ–°æ—¶é—´"].fillna("æœªçŸ¥")
    df["æˆªæ­¢æ—¶é—´"] = df["æˆªæ­¢æ—¶é—´"].fillna("æœªçŸ¥")

    # 3. å¯¹ä¸é‡è¦çš„æ•°æ®è¿›è¡Œç©ºå€¼å¡«å……
    df["å®˜æ–¹å…¬å‘Š"] = df["å®˜æ–¹å…¬å‘Š"].fillna("")
    df["æŠ•é€’æ–¹å¼"] = df["æŠ•é€’æ–¹å¼"].fillna("")
    df["å†…æ¨ç |å¤‡æ³¨"] = df["å†…æ¨ç |å¤‡æ³¨"].fillna("")

    # 4. é‡æ’åºå·ï¼ˆä¿è¯å”¯ä¸€ & è¿ç»­ï¼‰
    df["åºå·"] = range(1, len(df) + 1)
    return df


# æ„å»ºæ–‡æ¡£å’Œå…ƒæ•°æ®
def build_documents(df: pd.DataFrame):
    """å°†å²—ä½æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬ä¸å…ƒæ•°æ®é›†åˆã€‚

    Args:
        df (pd.DataFrame): ç»“æ„åŒ–å²—ä½æ•°æ®ã€‚

    Returns:
        tuple[list[str], list[dict], list[str]]: æ–‡æ¡£å†…å®¹ã€å…ƒæ•°æ®ä¸åŸå§‹ IDã€‚
    """

    # å°†å²—ä½æ•°æ®è½¬ä¸º documents + metadata + ids
    documents, metadatas, ids = [], [], []

    for _, row in df.iterrows():
        # æ‹¼æ¥æˆç”¨äºå‘é‡åŒ–çš„æ–‡æœ¬å†…å®¹
        content = f"""
        å…¬å¸åç§°: {row["å…¬å¸åç§°"]}
        ä¼ä¸šæ€§è´¨: {row["ä¼ä¸šæ€§è´¨"]}
        è¡Œä¸šå¤§ç±»: {row["è¡Œä¸šå¤§ç±»"]}
        æ‹›è˜å¯¹è±¡: {row["æ‹›è˜å¯¹è±¡"]}
        æ‹›è˜å²—ä½: {row["æ‹›è˜å²—ä½"]}
        ç½‘ç”³çŠ¶æ€: {row["ç½‘ç”³çŠ¶æ€"]}
        æŠ•é€’æ–¹å¼: {row["æŠ•é€’æ–¹å¼"]}
        """

        metadata = {
            "job_id": f"job_{row['åºå·']}",
            "company": row["å…¬å¸åç§°"],
            "title": row["æ‹›è˜å²—ä½"],
            "batch": row["æ‰¹æ¬¡"],
            "industry": row["è¡Œä¸šå¤§ç±»"],
            "location": row["å·¥ä½œåœ°ç‚¹"],
            "deadline": row["æˆªæ­¢æ—¶é—´"],
            "note": row["å†…æ¨ç |å¤‡æ³¨"],
        }

        ids.append(f"job_{row['åºå·']}")
        documents.append(content)
        metadatas.append(metadata)

    return documents, metadatas, ids

# åˆ†å—
def chunk_documents(documents: list, metadatas: list, ids: list):
    """å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—å¤„ç†ï¼Œç¡®ä¿å‘é‡åŒ–ç¨³å®šã€‚

    Args:
        documents (list): åŸå§‹å²—ä½æ–‡æœ¬åˆ—è¡¨ã€‚
        metadatas (list): ä¸æ–‡æœ¬å¯¹åº”çš„å…ƒæ•°æ®ã€‚
        ids (list): åŸå§‹å²—ä½ ID é›†åˆã€‚

    Returns:
        tuple[list[str], list[dict], list[str]]: åˆ†å—åçš„æ–‡æœ¬ã€å…ƒæ•°æ®ä¸åˆ†å— IDã€‚
    """

    # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†å—ä»¥å…å‡ºç°æ–‡æ¡£è¿‡é•¿å¯¼è‡´å‘é‡åŒ–å¤±è´¥
    splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    all_ids, all_documents, all_metadatas = [], [], []

    for i, document in enumerate(documents):
        chunks = splitter.split_text(document)
        for j, chunk in enumerate(chunks):
            base_id = metadatas[i]["job_id"]
            all_ids.append(f"{base_id}-{j}")
            all_documents.append(chunk)
            all_metadatas.append(metadatas[i])

    return all_documents, all_metadatas, all_ids

def persist_to_chroma(ids, documents, metadatas):
    """å†™å…¥å‘é‡åº“å¹¶æŒä¹…åŒ–æ•°æ®ã€‚

    Args:
        ids (list): æ–‡æ¡£åˆ†å— ID åˆ—è¡¨ã€‚
        documents (list): åˆ†å—æ–‡æœ¬å†…å®¹ã€‚
        metadatas (list): å¯¹åº”å…ƒæ•°æ®é›†åˆã€‚
    """

    persist_dir = Path(settings.chroma_persist_directory)
    tmp_dir = persist_dir.with_name(persist_dir.name + "_tmp")
    backup_dir = persist_dir.with_name(persist_dir.name + "_backup")

    if tmp_dir.exists():
        shutil.rmtree(tmp_dir)

    store = get_vector_store(persist_directory=str(tmp_dir))
    store.add_texts(texts=documents, metadatas=metadatas, ids=ids)
    store.persist()

    if backup_dir.exists():
        shutil.rmtree(backup_dir)
    if persist_dir.exists():
        shutil.move(str(persist_dir), str(backup_dir))

    shutil.move(str(tmp_dir), str(persist_dir))
    print(
        f"âœ… å·²å†™å…¥ {len(ids)} æ¡æ•°æ®åˆ° ChromaDB ({settings.chroma_collection_name})ï¼ŒåŸå§‹æ•°æ®å·²å¤‡ä»½åˆ° {backup_dir}"
    )

# æ•´åˆè¿è¡Œ
def run():
    """æ‰§è¡Œ ETL ä¸»æµç¨‹ï¼ŒåŒ…æ‹¬æ¸…æ´—ã€åˆ†å—ä¸æŒä¹…åŒ–ã€‚"""

    df = load_clean_data(settings.data_path)
    documents, metadatas, ids = build_documents(df)
    documents, metadatas, ids = chunk_documents(documents, metadatas, ids)

    document_count = len(documents)
    if document_count == 0:
        print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨æ–‡æ¡£ï¼ŒETL æµç¨‹æå‰ç»“æŸã€‚")
        return

    print("ğŸ’¾ å¼€å§‹å†™å…¥ ChromaDB ...")
    persist_to_chroma(ids, documents, metadatas)

if __name__ == "__main__":
    run()
